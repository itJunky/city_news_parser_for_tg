#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Crated by ITJunky

# –ü–æ–¥–≥—Ä—É–∂–∞–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É –¥–ª—è –æ–±—Ä–∞—â–µ–Ω–∏—è –∫ —Å–∞–π—Ç–∞–º
import requests

# –ü–æ–¥–≥—Ä—É–∂–∞–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–µ–∫—Å—Ç–∞
from bs4 import BeautifulSoup

# –ü–æ–¥–≥—Ä—É–∂–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
from config import durl
# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è, —Ä–µ–∞–ª–∏–∑—É—é—â–∞—è –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü
def parse_page(url='xx'):
  if url is 'xx':
    url = durl
  result = []
  # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
  page = requests.get(url).content
  # –ü–µ—Ä–µ–≤–∞—Ä–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ —É–¥–æ–±–Ω—ã–π –¥–ª—è —Ä–∞–∑–±–æ—Ä–∞ —Ñ–æ—Ä–º–∞—Ç xml
  soup = BeautifulSoup(page, 'lxml')

  for match in soup.findAll('span'):
    match.replace_with('')

  all_news = soup.findAll('div', {'class': 'kCrYT'})
  previous_url = ''
  for news in all_news:
    try:
      print(news.find('a')['href'])
      news_url = news.find('a')['href'].split('q=')[1].split('&sa=U')[0]
      print(news_url)
    except Exception as e:
      print(e)
      print('----\r\n' + str(news.find('a')))
      news_url = None

    print('\r\n\r\n')

    try:
      item = "üåç {} \n <a href='{}'>–ü–æ–¥—Ä–æ–±–Ω–µ–µ</a>".format(news.text, news_url)
    except Exception as e:
      print(e)
      item = None
    if previous_url == news_url or news_url is None:
      pass
    else:
      result.append(item)
      previous_url = news_url

  return result

# –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ —Å–∫—Ä–∏–ø—Ç–∞, –∑–∞–ø—É—Å–∫–∞—é—â–∏–π –≤—Å–µ —Ä–∞–Ω–µ–µ –æ–±—ä—è–≤–ª–µ–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
if __name__ == "__main__":
  print('Trying to parse url')
  print(parse_page())

